{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /mnt/lustre/scratch/CBRA/projects/CSVS/spanishTest/v2.0/machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "# sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cloucera/projects/spanishTest/.env')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv_file_path = Path(find_dotenv())\n",
    "dotenv_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/cloucera/projects/spanishTest')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = dotenv_file_path.parent\n",
    "project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/projects/spanishTest/v2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_file_path)\n",
    "data_path = Path(os.environ.get(\"DATA_PATH\"))\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "src_dir = os.path.join(project_path)\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG00096</th>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00097</th>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00099</th>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00100</th>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00101</th>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nationality continent\n",
       "HG00096     British  European\n",
       "HG00097     British  European\n",
       "HG00099     British  European\n",
       "HG00100     British  European\n",
       "HG00101     British  European"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_training_path = data_path.joinpath(\"ids_1000g.R\")\n",
    "y_train = pd.read_csv(labels_training_path, header=None, sep=\" \", index_col=0, names=[\"nationality\", \"continent\"])\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2504, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/data/projects/spanishTest/v2/plink.26.Q')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_training_path = data_path.joinpath(\"plink.26.Q\")\n",
    "features_training_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HG00096</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.339164</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00097</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.110726</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.393844</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00099</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.120551</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.358482</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00100</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.243461</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.065509</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HG00101</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.043336</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.287784</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1        2        3         4         5        6   \\\n",
       "HG00096  0.00001  0.00001  0.00001  0.00001  0.000010  0.000010  0.00001   \n",
       "HG00097  0.00001  0.00001  0.00001  0.00001  0.000010  0.110726  0.00001   \n",
       "HG00099  0.00001  0.00001  0.00001  0.00001  0.000010  0.120551  0.00001   \n",
       "HG00100  0.00001  0.00001  0.00001  0.00001  0.000010  0.000010  0.00001   \n",
       "HG00101  0.00001  0.00001  0.00001  0.00001  0.005575  0.000010  0.00001   \n",
       "\n",
       "              7        8        9   ...        16       17        18  \\\n",
       "HG00096  0.00001  0.00001  0.00001  ...  0.000010  0.00001  0.058456   \n",
       "HG00097  0.00001  0.00001  0.00001  ...  0.000010  0.00001  0.000010   \n",
       "HG00099  0.00001  0.00001  0.00001  ...  0.000010  0.00001  0.000010   \n",
       "HG00100  0.00001  0.00001  0.00001  ...  0.205808  0.00001  0.243461   \n",
       "HG00101  0.00001  0.00001  0.00001  ...  0.000010  0.00001  0.043336   \n",
       "\n",
       "               19       20       21       22        23        24       25  \n",
       "HG00096  0.078883  0.00001  0.00001  0.00001  0.339164  0.000010  0.00001  \n",
       "HG00097  0.000010  0.00001  0.00001  0.00001  0.000010  0.393844  0.00001  \n",
       "HG00099  0.000010  0.00001  0.00001  0.00001  0.358482  0.000010  0.00001  \n",
       "HG00100  0.000010  0.00001  0.00001  0.00001  0.065509  0.000010  0.00001  \n",
       "HG00101  0.000010  0.00001  0.00001  0.00001  0.050009  0.287784  0.00001  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(features_training_path, header=None, sep=\" \")\n",
    "X_train.index = y_train.index\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>nationality</th>\n",
       "      <th>continent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HG00096</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058456</td>\n",
       "      <td>0.078883</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.339164</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HG00097</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.110726</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.393844</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HG00099</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.120551</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.358482</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HG00100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243461</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.065509</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HG00101</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043336</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.287784</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>British</td>\n",
       "      <td>European</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        0        1        2        3         4         5        6  \\\n",
       "0  HG00096  0.00001  0.00001  0.00001  0.00001  0.000010  0.000010  0.00001   \n",
       "1  HG00097  0.00001  0.00001  0.00001  0.00001  0.000010  0.110726  0.00001   \n",
       "2  HG00099  0.00001  0.00001  0.00001  0.00001  0.000010  0.120551  0.00001   \n",
       "3  HG00100  0.00001  0.00001  0.00001  0.00001  0.000010  0.000010  0.00001   \n",
       "4  HG00101  0.00001  0.00001  0.00001  0.00001  0.005575  0.000010  0.00001   \n",
       "\n",
       "         7        8  ...        18        19       20       21       22  \\\n",
       "0  0.00001  0.00001  ...  0.058456  0.078883  0.00001  0.00001  0.00001   \n",
       "1  0.00001  0.00001  ...  0.000010  0.000010  0.00001  0.00001  0.00001   \n",
       "2  0.00001  0.00001  ...  0.000010  0.000010  0.00001  0.00001  0.00001   \n",
       "3  0.00001  0.00001  ...  0.243461  0.000010  0.00001  0.00001  0.00001   \n",
       "4  0.00001  0.00001  ...  0.043336  0.000010  0.00001  0.00001  0.00001   \n",
       "\n",
       "         23        24       25  nationality  continent  \n",
       "0  0.339164  0.000010  0.00001      British   European  \n",
       "1  0.000010  0.393844  0.00001      British   European  \n",
       "2  0.358482  0.000010  0.00001      British   European  \n",
       "3  0.065509  0.000010  0.00001      British   European  \n",
       "4  0.050009  0.287784  0.00001      British   European  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat((X_train, y_train), axis=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "# train_data.to_feather(data_path.joinpath(\"train_data.feather\"))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sptest import cli\n",
    "from sptest.datasets import load_training\n",
    "from sptest.model import SpanishPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.48s/it, best loss: 0.8034409792749403]\n"
     ]
    }
   ],
   "source": [
    "inputpath = None\n",
    "outputpath = \"xgb.skl\"\n",
    "features, labels = load_training(inputpath)\n",
    "tune = True\n",
    "ncpu = 8\n",
    "niters = 1\n",
    "\n",
    "model = SpanishPredictor(\n",
    "    tune=tune,\n",
    "    n_jobs=ncpu,\n",
    "    n_iter=niters\n",
    "    )\n",
    "model.fit(features, labels)\n",
    "model.save(outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model.estimator.get_booster()\n",
    "b.feature_names;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 26)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.47324723], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba_from_file(\"/home/cloucera/projects/spanishTest/sptest/data/AC5377.26.Q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "import numpy as np\n",
    "\n",
    "query = y_train.continent == \"European\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.repeat(True, y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(init=\"pca\", n_iter=10**4, metric=\"cosine\")\n",
    "X_t = tsne.fit_transform(X_train.loc[query, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_t, index=X_train.index[query], columns=[\"tsne_0\", \"tsne_1\"])\n",
    "data[\"nationality\"] = y_train.loc[query, \"nationality\"]\n",
    "data[\"continent\"] = y_train.loc[query, \"continent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nationality.unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclass = [\"Mexican-American\", \"Spanish\", \"Tuscan\", \"Puerto\", \"CEPH\"]\n",
    "\n",
    "def select_marker(key):\n",
    "    marker = \"s\" if key in missclass else \".\"\n",
    "    if key == \"Puerto\": marker = \"^\"\n",
    "    return marker\n",
    "\n",
    "symbols = {\n",
    "    key: select_marker(key)\n",
    "    for key in data.nationality.unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "sns.set_palette(\"tab20\", n_colors=26)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne_0\",\n",
    "    y=\"tsne_1\",\n",
    "    hue=\"nationality\",\n",
    "    style=\"nationality\",\n",
    "    markers=symbols,\n",
    "    data=data,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Put the legend out of the figure\n",
    "lgnd = plt.legend(bbox_to_anchor=(1.05, 1), borderaxespad=0., markerscale=2)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='14'); # for legend text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tpot.builtins import StackingEstimator, ZeroCount\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Average CV score on the training set was:0.9146285310055363\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    StackingEstimator(estimator=KNeighborsClassifier(n_neighbors=40, p=2, weights=\"uniform\")),\n",
    "    StackingEstimator(estimator=LogisticRegression(C=5.0, dual=False, penalty=\"l1\")),\n",
    "    RobustScaler(),\n",
    "    ZeroCount(),\n",
    "    PCA(iterated_power=4, svd_solver=\"randomized\"),\n",
    "    LogisticRegression(C=20.0, dual=False, penalty=\"l1\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# clf = ExtraTreeClassifier()\n",
    "# clf = RidgeClassifierCV()\n",
    "# clf = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xts, ytr, yts = train_test_split(X_train, y_train.nationality == \"Spanish\", test_size=0.3, stratify=y_train.nationality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"stacked\"\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "cm = ConfusionMatrix(clf, ax=ax)\n",
    "\n",
    "with ignore_warnings():\n",
    "    cm.fit(xtr, ytr)\n",
    "\n",
    "cm.score(xts, yts)\n",
    "cm.finalize()\n",
    "#     cm.poof()\n",
    "ax.set_title(\"{} Confusion Matrix\".format(name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "y_train_3 = [y if y in [\"Spanish\", \"Tuscan\"] else \"Other\" for y in y_train.nationality]\n",
    "\n",
    "xtr, xts, ytr, yts = train_test_split(X_train, y_train.nationality, test_size=0.3, stratify=y_train.nationality)\n",
    "\n",
    "classifiers = [\n",
    "    (LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\"), {\n",
    "        'C': np.logspace(-2, 7, 10)\n",
    "    }),\n",
    "    (LinearSVC(random_state=0, class_weight=\"balanced\"), {\n",
    "        'C': np.logspace(-2, 7, 10)\n",
    "    }),\n",
    "    (BalancedRandomForestClassifier(n_estimators=500, random_state=0), {\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    }),\n",
    "    (SVC(random_state=0, gamma='auto', class_weight=\"balanced\"), {\n",
    "        'C': np.logspace(-2, 7, 10)\n",
    "    }),\n",
    "]\n",
    "\n",
    "def get_name(estimator):\n",
    "    name = estimator.__class__.__name__\n",
    "    if name == 'Pipeline':\n",
    "        name = [get_name(est[1]) for est in estimator.steps]\n",
    "        name = ' + '.join(name)\n",
    "    return name\n",
    "\n",
    "names = [get_name(e) for e, g in classifiers]\n",
    "\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "for est_idx, (name, (estimator, param_grid)) in enumerate(zip(names, classifiers)):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, iid=False)\n",
    "\n",
    "    cm = ConfusionMatrix(clf, ax=ax)\n",
    "    \n",
    "    with ignore_warnings(category=ConvergenceWarning):\n",
    "        cm.fit(xtr, ytr)\n",
    "\n",
    "    cm.score(xts, yts)\n",
    "    cm.finalize()\n",
    "#     cm.poof()\n",
    "    ax.set_title(\"{} Confusion Matrix\".format(name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm = make_pipeline(\n",
    "    SVC(C=1000.0, cache_size=200, coef0=0.0,\n",
    "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
    "  tol=0.001, verbose=False)\n",
    ")\n",
    "\n",
    "clm.fit(xtr, ytr)\n",
    "\n",
    "print(classification_report(ytr, clm.predict(xtr)))\n",
    "print(classification_report(yts, clm.predict(xts)))\n",
    "\n",
    "cross_val_score(clm, xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_true = np.array(y_train_3)\n",
    "y_hat = y_true.copy()\n",
    "y_hat[y_true == \"Other\"] = \"Tuscan\"\n",
    "y_hat[y_true == \"Tuscan\"] = \"Other\"\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from autoxgb import OptimizedXGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sptest.zero_count import ZeroCount\n",
    "from sptest.stacking_estimator import StackingEstimator\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def build_default_model(seed=42):\n",
    "\n",
    "    estimator = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        StackingEstimator(estimator=KNeighborsClassifier(n_neighbors=40,\n",
    "                                                         p=2,\n",
    "                                                         weights=\"uniform\")),\n",
    "        StackingEstimator(estimator=LogisticRegression(C=5.0,\n",
    "                                                       dual=False,\n",
    "                                                       penalty=\"l1\",\n",
    "                                                       random_state=seed,\n",
    "                                                       max_iter=10**4,\n",
    "                                                       solver=\"saga\")),\n",
    "        RobustScaler(),\n",
    "        ZeroCount(),\n",
    "        PCA(iterated_power=4, svd_solver=\"randomized\", random_state=seed),\n",
    "        LogisticRegression(\n",
    "            C=20.0, dual=False, penalty=\"l1\", random_state=seed, max_iter=10**4, solver=\"saga\")\n",
    "    )\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spclf = build_default_model()\n",
    "\n",
    "y_train_bin = y_train[\"nationality\"].values.ravel() == \"Spanish\"\n",
    "spclf.fit(X_train, y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(spclf, \"default_model.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, xts, ytr, yts =  train_test_split(\n",
    "    X_train, \n",
    "    y_train[\"nationality\"].values.ravel() == \"Spanish\", \n",
    "    test_size=0.3,\n",
    "    stratify=y_train[\"nationality\"].values.ravel() == \"Spanish\"\n",
    ")\n",
    "xtr.shape, xts.shape, ytr.shape, yts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = OptimizedXGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "xgb.fit(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr_hat = xgb.predict(xtr)\n",
    "proba_train = xgb.predict_proba(xtr)\n",
    "\n",
    "yts_hat = xgb.predict(xts)\n",
    "proba_test = xgb.predict_proba(xts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(yts, yts_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.externals.funcsigs import signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(proba, y, split):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "    from sklearn.externals.funcsigs import signature\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y, proba[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    average_precision = average_precision_score(y, proba[:, 1])\n",
    "    precision, recall, _ = precision_recall_curve(y, proba[:, 1])\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "    # ROC\n",
    "    lw = 2\n",
    "    axes[0].plot(fpr, tpr, color='darkorange',\n",
    "            lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    axes[0].plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title(\"ROC\")\n",
    "    axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                if 'step' in signature(plt.fill_between).parameters\n",
    "                else {})\n",
    "    axes[1].step(recall, precision, color='b', alpha=0.2,\n",
    "            where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    # P-R curve\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "    plt.suptitle(split)\n",
    "    # plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(proba_train, ytr, split=\"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(proba_test, yts, split=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(*proba_test.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dmc = DummyClassifier()\n",
    "dmc.fit(xtr, ytr)\n",
    "dyts = dmc.predict_proba(xts)\n",
    "\n",
    "plot_model(dyts, yts, split=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = xgb.best_estimator_\n",
    "\n",
    "cross_val_score(clf, X_train, y_train[\"nationality\"].values.ravel() == \"Spanish\", cv=10, n_jobs=-1, scoring=\"average_precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from tpot.builtins import StackingEstimator, ZeroCount\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import DotProduct\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "kernel=DotProduct()\n",
    "\n",
    "# Average CV score on the training set was:0.9146285310055363\n",
    "spclf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    StackingEstimator(estimator=KNeighborsClassifier(n_neighbors=40, p=2, weights=\"uniform\")),\n",
    "    StackingEstimator(estimator=LogisticRegression(C=5.0, dual=False, penalty=\"l1\", max_iter=10**4)),\n",
    "    RobustScaler(),\n",
    "    ZeroCount(),\n",
    "    PCA(iterated_power=4, svd_solver=\"randomized\"),\n",
    "    LogisticRegression(C=20.0, dual=False, penalty=\"l1\", max_iter=10**4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.nationality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = y_train.nationality.str.contains(\"Spanish|Tuscan\").values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.nationality[y_train_bin].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.loc[y_train_bin, :]\n",
    "y_train_bin_ = y_train.nationality[y_train_bin].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(BalancedRandomForestClassifier(n_estimators=10**3, n_jobs=-1), X_train, y_train_bin, cv=10, n_jobs=-1, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train_, y_train_bin_, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spclf = build_default_model()\n",
    "\n",
    "y_train_bin = y_train[\"nationality\"].values.ravel() == \"Spanish\"\n",
    "spclf.fit(X_train, y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(spclf, \"default_model.skl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "y_train_bin = y_train[\"nationality\"].values.ravel() == \"Tuscan\"\n",
    "\n",
    "visualizer = FeatureCorrelation(method='mutual_info-classification', labels=X_train.columns)\n",
    "visualizer.fit(X_train, y_train_bin)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from yellowbrick.target import FeatureCorrelation\n",
    "\n",
    "y_train_bin = y_train[\"nationality\"].values.ravel() == \"Spanish\"\n",
    "\n",
    "visualizer = FeatureCorrelation(method='mutual_info-classification', labels=X_train.columns)\n",
    "visualizer.fit(X_train, y_train_bin)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = FeatureCorrelation(labels=X_train.columns)\n",
    "visualizer.fit(X_train, y_train_bin)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = data_path.joinpath(\"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "sample_names = []\n",
    "frames = []\n",
    "for fpath in data_path.glob(\"**/*.Q\"):\n",
    "#     print(fpath)\n",
    "    sample_name = fpath.parent.name\n",
    "    group = fpath.parent.parent.name\n",
    "    \n",
    "    df = pd.read_csv(fpath, header=None, sep=\" \")\n",
    "    \n",
    "    if df.shape[0] >= 1:\n",
    "        groups.append(group)\n",
    "        sample_names.append(sample_name)\n",
    "        frames.append(df.iloc[0, :].copy())        \n",
    "    \n",
    "test_v2 = pd.concat(frames, axis=1, ignore_index=True).T\n",
    "test_v2.index = sample_names\n",
    "test_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = test_v2.copy()\n",
    "v2[\"groups\"] = groups\n",
    "v2[v2.groups.str.contains(\"Anti\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results = pd.DataFrame(\n",
    "    {\"is_spanish\": spclf.predict(test_v2), \"proba_spanish\": spclf.predict_proba(test_v2)[:, 1], \"group\": groups},\n",
    "    index=test_v2.index\n",
    ")\n",
    "\n",
    "test_v2_results.group = test_v2_results.group.str.split(\"_\").str[0]\n",
    "test_v2_results.drop(index=[\"v2\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results.groupby(\"is_spanish\").plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results.is_spanish.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results.to_csv(\"test_v2_result_stacking.tsv\", sep=\"\\t\", index=True, index_label=\"sample_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "sns.boxplot(x=\"group\", y=\"proba_spanish\", data=test_v2_results, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=70); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results.loc[test_v2_results.group.str.contains(\"Rosario\"), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results[~test_v2_results.is_spanish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_v2 = [\"IIB-401\", \"IIB-412\", \"IIB-52\", \"IIB-198\"]\n",
    "test_v2_results.loc[query_v2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_v2 = [\"AC5399\", \"AC5378\", \"AC5390\", \"AC5409\", \"AC5415\", \"AC5532\", \"AC5533\"]\n",
    "\n",
    "test_v2_results.loc[query_v2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v2_results[test_v2_results.proba_spanish < 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "fig, axs = plt.subplots(7, 4, figsize=(22, 6*7), facecolor='w', edgecolor='k')\n",
    "axs = axs.ravel()\n",
    "\n",
    "df = X_train.loc[y_train_bin, :]\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    isolation_forest = IsolationForest(contamination='auto', behaviour=\"new\")\n",
    "    isolation_forest.fit(df[column].values.reshape(-1,1))\n",
    "\n",
    "    xx = np.linspace(X_train[column].min(), X_train[column].max(), len(X_train)).reshape(-1,1)\n",
    "    anomaly_score = isolation_forest.decision_function(xx)\n",
    "    outlier = isolation_forest.predict(xx)\n",
    "    \n",
    "    axs[i].plot(xx, anomaly_score, label='anomaly score')\n",
    "    axs[i].fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score), \n",
    "                     where=outlier==-1, color='r', \n",
    "                     alpha=.4, label='outlier region')\n",
    "    axs[i].legend()\n",
    "    axs[i].set_title(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "clust = LocalOutlierFactor(n_neighbors=50, contamination='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# scale data first\n",
    "X = StandardScaler().fit_transform(df)\n",
    "\n",
    "db = clust.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(y_train.nationality.values, y_train.labels, rownames=['truth'], colnames=['pred'], margins=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_path = data_path.parent.joinpath(\"v3\")\n",
    "v3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_projections_path = v3_path.joinpath(\"projections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = []\n",
    "frames = []\n",
    "\n",
    "for fpath in v3_projections_path.glob(\"**/*.Q\"):\n",
    "#     print(fpath)\n",
    "    sample_name = fpath.parent.name\n",
    "    \n",
    "    df = pd.read_csv(fpath, header=None, sep=\" \")\n",
    "    \n",
    "    if df.shape[0] >= 1:\n",
    "        sample_names.append(sample_name)\n",
    "        frames.append(df.iloc[0, :].copy())        \n",
    "    \n",
    "test_v3 = pd.concat(frames, axis=1, ignore_index=True).T\n",
    "test_v3.index = sample_names\n",
    "test_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_train_path = v3_path.joinpath(\"plink.3.Q\")\n",
    "v3_metadata_path = v3_path.joinpath(\"samples\")\n",
    "\n",
    "v3_metadata = pd.read_csv(v3_metadata_path, header=None, sep=\"\\t\", names=[\"class\", \"index\"], index_col=1)\n",
    "\n",
    "v3_train = pd.read_csv(v3_train_path, header=None, sep=\" \")\n",
    "v3_train.index = v3_metadata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_tsne = pd.DataFrame(manifold.TSNE(random_state=42).fit_transform(v3_train))\n",
    "v3_tsne.plot(kind=\"scatter\", x=0, y=1, figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_train.plot(style=\".\", subplots=True, figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v3.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_v3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# fit the model\n",
    "clf = IsolationForest(behaviour='new', max_samples=100,\n",
    "                      random_state=rng, contamination='auto')\n",
    "clf.fit(v3_train)\n",
    "y_pred_train = clf.predict(v3_train)\n",
    "y_pred_test = clf.predict(test_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = v3_metadata.loc[v3_train.index, \"class\"] == \"IBS\"\n",
    "data_filt = v3_train.loc[query, :]\n",
    "\n",
    "v3_tsne = pd.DataFrame(manifold.TSNE(random_state=42).fit_transform(data_filt), columns=[\"tsne0\", \"tsne1\"], index=data_filt.index)\n",
    "# v3_tsne[\"inlier\"] = data_filt\n",
    "v3_tsne[\"class\"] = v3_metadata.loc[query, \"class\"].values\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.scatterplot(x=\"tsne0\", y=\"tsne1\", data=v3_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_train.loc[\"HG01502\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_metadata[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_tsne.inlier.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index.isin(v3_train.index).sum(), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_data = v3_train.copy()\n",
    "v3_data[\"class\"] = v3_metadata.loc[v3_train.index, \"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = v3_data[\"class\"] == \"IBS\"\n",
    "v3_data.loc[query, 1].plot(style=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  v3_data[\"class\"] == \"TSI\"\n",
    "v3_data.loc[query, 1].plot(style=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query =  v3_data[\"class\"] == \"NAV\"\n",
    "v3_data.loc[query, :].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3_train.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train_bin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lof = X_train.loc[y_train_bin, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=5, novelty=True, contamination=0.01)\n",
    "clf.fit(X_train_lof)\n",
    "pd.Series(clf.predict(X_train.loc[~y_train_bin, :])).value_counts(), sum(y_train_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(clf.predict(test_v2)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(clf.decision_function(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"fun\": spclf.decision_function(X_train), \"nation\": y_train.nationality, \"proba\":spclf.predict_proba(X_train)[:, 1]}, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "sns.boxplot(x=\"nation\", y=\"fun\", data=df1, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"fun\": clf.decision_function(test_v2), \n",
    "        \"proba\": spclf.predict_proba(test_v2)[:, 1],\n",
    "        \"clf\": clf.predict(test_v2),\n",
    "        \"group\": groups\n",
    "    }, \n",
    "    index=test_v2.index)\n",
    "df2[\"nation\"] = df2.group\n",
    "df = pd.concat((df1, df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "sns.boxplot(x=\"nation\", y=\"fun\", data=df, ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_pred = pd.Series(clf.predict(test_v2), index=test_v2.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_v2 = [\"AC5399\", \"AC5378\", \"AC5390\", \"AC5409\", \"AC5415\", \"AC5532\", \"AC5533\"]\n",
    "df2.loc[query_v2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df2.proba > 0.9) & (df2.clf == -1)\n",
    "df2.loc[query, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = df.groupby(\"nation\").describe().loc[[\"v2\", \"Spanish\"], :]\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ((df2.fun < dfs.fun[\"25%\"][\"Spanish\"]) | (df2.fun > dfs.fun[\"75%\"][\"Spanish\"])) & (df2.proba > 0.99)\n",
    "print(sum(query))\n",
    "df2.loc[query, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df2.fun < dfs.fun[\"25%\"][\"Spanish\"])  & (df2.proba > 0.99)\n",
    "print(sum(query))\n",
    "df2.loc[query, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"nation\").describe().loc[[\"v2\", \"Spanish\"], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [x in [\"Spanish\", \"Tuscan\"] for x in ytr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_q = xtr.loc[query, :]\n",
    "ytr_q = ytr[query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "#xtr_q = KBinsDiscretizer(n_bins=2).fit_transform(xtr_q).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = manifold.TSNE(init=\"pca\", n_iter=10**4, metric=distance.correlation)\n",
    "X_t = tsne.fit_transform(xtr_q)\n",
    "\n",
    "data = pd.DataFrame(X_t, index=xtr.loc[query, :].index, columns=[\"tsne_0\", \"tsne_1\"])\n",
    "data[\"nationality\"] = ytr_q\n",
    "data[\"continent\"] = ytr_q\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "sns.set_palette(\"colorblind\", n_colors=2)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne_0\",\n",
    "    y=\"tsne_1\",\n",
    "    hue=\"nationality\",\n",
    "    style=\"nationality\",\n",
    "    data=data,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Put the legend out of the figure\n",
    "lgnd = plt.legend(bbox_to_anchor=(1.05, 1), borderaxespad=0., markerscale=2)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='14'); # for legend text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ts = [x in [\"Spanish\", \"Tuscan\"] for x in yts]\n",
    "xts_q = xts.loc[query_ts, :]\n",
    "yts_q = yts[query_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xts_q = KBinsDiscretizer(n_bins=2).fit_transform(xts_q).todense()\n",
    "\n",
    "tsne = manifold.TSNE(init=\"pca\", n_iter=10**4, metric=distance.correlation)\n",
    "X_t = tsne.fit_transform(xts_q)\n",
    "\n",
    "data = pd.DataFrame(X_t, index=xts.loc[query_ts, :].index, columns=[\"tsne_0\", \"tsne_1\"])\n",
    "data[\"nationality\"] = yts_q\n",
    "data[\"continent\"] = yts_q\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "sns.set_palette(\"colorblind\", n_colors=2)\n",
    "sns.scatterplot(\n",
    "    x=\"tsne_0\",\n",
    "    y=\"tsne_1\",\n",
    "    hue=\"nationality\",\n",
    "    style=\"nationality\",\n",
    "    data=data,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Put the legend out of the figure\n",
    "lgnd = plt.legend(bbox_to_anchor=(1.05, 1), borderaxespad=0., markerscale=2)\n",
    "plt.setp(ax.get_legend().get_texts(), fontsize='14'); # for legend text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_old = 30*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_now = 30*4 + 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_now * (1005 / total_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1005 + 1585 + 1427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19 * (1892 / 30 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\n",
    "ITERATIONS = 10 # 1000\n",
    "\n",
    "# Split into X and y\n",
    "y = y_train_bin.copy()\n",
    "X = X_train.copy()\n",
    "\n",
    "\n",
    "# Classifier\n",
    "bayes_cv_tuner = BayesSearchCV(\n",
    "    estimator = xgb.XGBClassifier(\n",
    "        n_jobs = 7,\n",
    "        objective = 'binary:logistic',\n",
    "        eval_metric = 'aucpr',\n",
    "        silent=1,\n",
    "        tree_method='approx'\n",
    "    ),\n",
    "    search_spaces = {\n",
    "        'learning_rate': (0.01, 1.0, 'log-uniform'),\n",
    "        'min_child_weight': (0, 10),\n",
    "        'max_depth': (0, 50),\n",
    "        'max_delta_step': (0, 20),\n",
    "        'subsample': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bytree': (0.01, 1.0, 'uniform'),\n",
    "        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n",
    "        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n",
    "        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n",
    "        'gamma': (1e-9, 0.5, 'log-uniform'),\n",
    "        'min_child_weight': (0, 5),\n",
    "        'n_estimators': (50, 100),\n",
    "        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n",
    "    },    \n",
    "    scoring = 'average_precision',\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=3,\n",
    "        shuffle=True,\n",
    "        random_state=42\n",
    "    ),\n",
    "    n_jobs = 7,\n",
    "    n_iter = ITERATIONS,   \n",
    "    verbose = 0,\n",
    "    refit = True,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "def status_print(optim_result):\n",
    "    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n",
    "    \n",
    "    # Get all the models tested so far in DataFrame format\n",
    "    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n",
    "    \n",
    "    # Get current parameters and the best parameters    \n",
    "    best_params = pd.Series(bayes_cv_tuner.best_params_)\n",
    "    print('Model #{}\\nBest PR-AUC: {}\\nBest params: {}\\n'.format(\n",
    "        len(all_models),\n",
    "        np.round(bayes_cv_tuner.best_score_, 4),\n",
    "        bayes_cv_tuner.best_params_\n",
    "    ))\n",
    "    \n",
    "    # Save all model results\n",
    "    clf_name = bayes_cv_tuner.estimator.__class__.__name__\n",
    "    all_models.to_csv(clf_name+\"_cv_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = bayes_cv_tuner.fit(X.values, y, callback=status_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
